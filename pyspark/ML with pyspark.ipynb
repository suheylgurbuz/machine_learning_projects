{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/suheylgurbuz/spark-2.4.5-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "# Import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the cluster\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mon', 'int'),\n",
       " ('dom', 'int'),\n",
       " ('dow', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('org', 'string'),\n",
       " ('mile', 'int'),\n",
       " ('depart', 'double'),\n",
       " ('duration', 'int'),\n",
       " ('delay', 'int')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv(\"sms.csv\", sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+\n",
      "|  0| 22|  2|     UA|ORD| 316| 16.33|      82|   30|    1|\n",
      "|  2| 20|  4|     UA|SFO| 337|  6.17|      82|   -8|    0|\n",
      "|  9| 13|  1|     AA|ORD|1236| 10.33|     195|   -5|    0|\n",
      "|  5|  2|  1|     UA|SFO| 550|  7.98|     102|    2|    0|\n",
      "|  7|  2|  6|     AA|ORD| 733| 10.83|     135|   54|    1|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_km = flights_none_missing.withColumn('km', round(flights.mile * 1.60934, 0))\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_label = flights_none_missing.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 316| 16.33|      82|   30|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO| 337|  6.17|      82|   -8|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD|1236| 10.33|     195|   -5|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO| 550|  7.98|     102|    2|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 733| 10.83|     135|   54|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights_label)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_label)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,316.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,337.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1236.0,10.33,195.0]|-5   |\n",
      "|[5.0,2.0,1.0,0.0,1.0,550.0,7.98,102.0]   |2    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,733.0,10.83,135.0]  |54   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'mile', 'depart', 'duration',\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+-----------------------------------------+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|carrier_idx|org_idx|features                                 |\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+-----------------------------------------+\n",
      "|0  |22 |2  |UA     |ORD|316 |16.33 |82      |30   |1    |0.0        |0.0    |[0.0,22.0,2.0,0.0,0.0,316.0,16.33,82.0]  |\n",
      "|2  |20 |4  |UA     |SFO|337 |6.17  |82      |-8   |0    |0.0        |1.0    |[2.0,20.0,4.0,0.0,1.0,337.0,6.17,82.0]   |\n",
      "|9  |13 |1  |AA     |ORD|1236|10.33 |195     |-5   |0    |1.0        |0.0    |[9.0,13.0,1.0,1.0,0.0,1236.0,10.33,195.0]|\n",
      "|5  |2  |1  |UA     |SFO|550 |7.98  |102     |2    |0    |0.0        |1.0    |[5.0,2.0,1.0,0.0,1.0,550.0,7.98,102.0]   |\n",
      "|7  |2  |6  |AA     |ORD|733 |10.83 |135     |54   |1    |1.0        |0.0    |[7.0,2.0,6.0,1.0,0.0,733.0,10.83,135.0]  |\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_assembled.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980732423121092\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights_assembled.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------------------------+\n",
      "|label|prediction|probability                            |\n",
      "+-----+----------+---------------------------------------+\n",
      "|0    |1.0       |[0.3530523876318839,0.6469476123681162]|\n",
      "|0    |1.0       |[0.3530523876318839,0.6469476123681162]|\n",
      "|1    |1.0       |[0.3530523876318839,0.6469476123681162]|\n",
      "|1    |1.0       |[0.3530523876318839,0.6469476123681162]|\n",
      "|1    |1.0       |[0.3530523876318839,0.6469476123681162]|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1611|\n",
      "|    0|       0.0| 2835|\n",
      "|    1|       1.0| 3197|\n",
      "|    0|       1.0| 1852|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.6352817272248552\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1695|\n",
      "|    0|       0.0| 2670|\n",
      "|    1|       1.0| 3113|\n",
      "|    0|       1.0| 2017|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.63\n",
      "recall    = 0.66\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,344,378,1006],[2.2391682769656747,2.892706319430574,3.684405173719015,4.244020961654438])|\n",
      "|[dont, worry, guess, busy]      |(1024,[53,233,329,858],[4.618714411095849,3.557143394108088,4.618714411095849,4.937168142214383])   |\n",
      "|[call, freephone]               |(1024,[138,396],[2.2391682769656747,3.3843005812686773])                                            |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,69,387,428],[3.7897656893768414,7.284881949239966,4.4671645129686475,3.898659777615979])  |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
    "      .transform(wrangled)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024)\\\n",
    "      .transform(wrangled)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
    "      .fit(wrangled).transform(wrangled)\n",
    "      \n",
    "tf_idf.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   47|\n",
      "|    0|       0.0|  987|\n",
      "|    1|       1.0|  124|\n",
      "|    0|       1.0|    3|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "tf_idf_train, tf_idf_test = tf_idf.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(tf_idf_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(tf_idf_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SMF|    4.0|(7,[4],[1.0])|\n",
      "|SJC|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights_assembled)\n",
    "flights_onehot = onehot.transform(flights_assembled)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|carrier_idx|org_idx|            features|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+\n",
      "|  0| 22|  2|     UA|ORD| 316| 16.33|      82|   30|    1|        0.0|    0.0|[0.0,22.0,2.0,0.0...|\n",
      "|  2| 20|  4|     UA|SFO| 337|  6.17|      82|   -8|    0|        0.0|    1.0|[2.0,20.0,4.0,0.0...|\n",
      "|  9| 13|  1|     AA|ORD|1236| 10.33|     195|   -5|    0|        1.0|    0.0|[9.0,13.0,1.0,1.0...|\n",
      "|  5|  2|  1|     UA|SFO| 550|  7.98|     102|    2|    0|        0.0|    1.0|[5.0,2.0,1.0,0.0,...|\n",
      "|  7|  2|  6|     AA|ORD| 733| 10.83|     135|   54|    1|        1.0|    0.0|[7.0,2.0,6.0,1.0,...|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_assembled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|carrier_idx|org_idx|            features|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+\n",
      "|  0|  1|  2|     AA|JFK|1597|  6.58|     230|   50|    1|        1.0|    2.0|[0.0,1.0,2.0,1.0,...|\n",
      "|  0|  1|  2|     AA|JFK|2475|  12.0|     370|   11|    0|        1.0|    2.0|[0.0,1.0,2.0,1.0,...|\n",
      "|  0|  1|  2|     AA|JFK|2475|  17.0|     379|  -10|    0|        1.0|    2.0|[0.0,1.0,2.0,1.0,...|\n",
      "|  0|  1|  2|     AA|JFK|2586|   7.0|     385|  -16|    0|        1.0|    2.0|[0.0,1.0,2.0,1.0,...|\n",
      "|  0|  1|  2|     AA|LGA| 733| 14.58|     165|   -4|    0|        1.0|    3.0|[0.0,1.0,2.0,1.0,...|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37527\n"
     ]
    }
   ],
   "source": [
    "print(flights_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 316| 16.33|      82|   30|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO| 337|  6.17|      82|   -8|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD|1236| 10.33|     195|   -5|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO| 550|  7.98|     102|    2|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 733| 10.83|     135|   54|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|features|duration|\n",
      "+--------+--------+\n",
      "|[316.0] |82      |\n",
      "|[337.0] |82      |\n",
      "|[1236.0]|195     |\n",
      "|[550.0] |102     |\n",
      "|[733.0] |135     |\n",
      "+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.7980732423121092\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler_2 = VectorAssembler(inputCols=['mile'], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled_2 = assembler_2.transform(flights_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled_2.select('features', 'duration').show(5, truncate=False)\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train_2, flights_test_2 = flights_assembled_2.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio_2 = flights_train_2.count() / flights_assembled_2.count()\n",
    "print(training_ratio_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|170     |133.3918900459809 |\n",
      "|123     |131.68655178239865|\n",
      "|135     |149.71441342598226|\n",
      "|275     |268.9662820007684 |\n",
      "|275     |268.9662820007684 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.046852716017344"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train_2)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test_2)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.105250959853976\n",
      "[0.12180987597015949]\n",
      "0.12180987597015949\n",
      "492.5708980665785\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+-------------+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|carrier_idx|org_idx|            features|    org_dummy|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+-------------+\n",
      "|  0| 22|  2|     UA|ORD| 316| 16.33|      82|   30|    1|        0.0|    0.0|[0.0,22.0,2.0,0.0...|(7,[0],[1.0])|\n",
      "|  2| 20|  4|     UA|SFO| 337|  6.17|      82|   -8|    0|        0.0|    1.0|[2.0,20.0,4.0,0.0...|(7,[1],[1.0])|\n",
      "|  9| 13|  1|     AA|ORD|1236| 10.33|     195|   -5|    0|        1.0|    0.0|[9.0,13.0,1.0,1.0...|(7,[0],[1.0])|\n",
      "|  5|  2|  1|     UA|SFO| 550|  7.98|     102|    2|    0|        0.0|    1.0|[5.0,2.0,1.0,0.0,...|(7,[1],[1.0])|\n",
      "|  7|  2|  6|     AA|ORD| 733| 10.83|     135|   54|    1|        1.0|    0.0|[7.0,2.0,6.0,1.0,...|(7,[0],[1.0])|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_onehot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_onehot_2 = flights_onehot.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+-------------+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|label|carrier_idx|org_idx|    org_dummy|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+-------------+\n",
      "|  0| 22|  2|     UA|ORD| 316| 16.33|      82|   30|    1|        0.0|    0.0|(7,[0],[1.0])|\n",
      "|  2| 20|  4|     UA|SFO| 337|  6.17|      82|   -8|    0|        0.0|    1.0|(7,[1],[1.0])|\n",
      "|  9| 13|  1|     AA|ORD|1236| 10.33|     195|   -5|    0|        1.0|    0.0|(7,[0],[1.0])|\n",
      "|  5|  2|  1|     UA|SFO| 550|  7.98|     102|    2|    0|        0.0|    1.0|(7,[1],[1.0])|\n",
      "|  7|  2|  6|     AA|ORD| 733| 10.83|     135|   54|    1|        1.0|    0.0|(7,[0],[1.0])|\n",
      "+---+---+---+-------+---+----+------+--------+-----+-----+-----------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_onehot_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------+\n",
      "|features              |duration|\n",
      "+----------------------+--------+\n",
      "|(8,[0,1],[316.0,1.0]) |82      |\n",
      "|(8,[0,2],[337.0,1.0]) |82      |\n",
      "|(8,[0,1],[1236.0,1.0])|195     |\n",
      "|(8,[0,2],[550.0,1.0]) |102     |\n",
      "|(8,[0,1],[733.0,1.0]) |135     |\n",
      "+----------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.7980732423121092\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler_3 = VectorAssembler(inputCols=['mile', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled_3 = assembler_3.transform(flights_onehot_2)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled_3.select('features', 'duration').show(5, truncate=False)\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train_3, flights_test_3 = flights_assembled_3.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio_3 = flights_train_3.count() / flights_assembled_3.count()\n",
    "print(training_ratio_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.154457617987484"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression_2 = LinearRegression(labelCol='duration').fit(flights_train_3)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions_2 = regression_2.transform(flights_test_3)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+-------+\n",
      "|duration|mile|org|org_idx|\n",
      "+--------+----+---+-------+\n",
      "|      82| 316|ORD|    0.0|\n",
      "|      82| 337|SFO|    1.0|\n",
      "|     195|1236|ORD|    0.0|\n",
      "|     102| 550|SFO|    1.0|\n",
      "|     135| 733|ORD|    0.0|\n",
      "+--------+----+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_reshaped = flights_indexed['duration', 'mile', 'org', 'org_idx']\n",
    "flights_reshaped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------------+----+------------------+-----------------+-----+-----+---+\n",
      "|duration|              JFK|              LGA| OGG|               ORD|              SFO|  SJC|  SMF|TUS|\n",
      "+--------+-----------------+-----------------+----+------------------+-----------------+-----+-----+---+\n",
      "|     148|            548.5|685.8823529411765| 0.0| 810.3333333333334|954.6923076923077|948.0|910.0|0.0|\n",
      "|     243|           1522.0|           1619.0| 0.0|1448.3333333333333|           1846.0|  0.0|  0.0|0.0|\n",
      "|     392|2551.733333333333|              0.0| 0.0|               0.0|              0.0|  0.0|  0.0|0.0|\n",
      "|     540|              0.0|              0.0| 0.0|            4243.0|              0.0|  0.0|  0.0|0.0|\n",
      "|      31|              0.0|              0.0|84.0|               0.0|              0.0|  0.0|  0.0|0.0|\n",
      "+--------+-----------------+-----------------+----+------------------+-----------------+-----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_reshaped = flights_reshaped.groupby('duration').pivot('org').avg('mile').fillna(0)\n",
    "flights_reshaped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n"
     ]
    }
   ],
   "source": [
    "print(flights_reshaped.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------+--------+\n",
      "|features                                                                         |duration|\n",
      "+---------------------------------------------------------------------------------+--------+\n",
      "|[548.5,685.8823529411765,0.0,810.3333333333334,954.6923076923077,948.0,910.0,0.0]|148     |\n",
      "|(8,[0,1,3,4],[1522.0,1619.0,1448.3333333333333,1846.0])                          |243     |\n",
      "|(8,[0],[2551.733333333333])                                                      |392     |\n",
      "|(8,[3],[4243.0])                                                                 |540     |\n",
      "|(8,[2],[84.0])                                                                   |31      |\n",
      "+---------------------------------------------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.797979797979798\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler_4 = VectorAssembler(inputCols=['JFK', 'LGA', 'OGG', 'ORD', 'SFO', 'SJC', 'SMF', 'TUS'], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled_4 = assembler_4.transform(flights_reshaped)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled_4.select('features', 'duration').show(5, truncate=False)\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train_4, flights_test_4 = flights_assembled_4.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio_4 = flights_train_4.count() / flights_assembled_4.count()\n",
    "print(training_ratio_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.82023114712439"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression_3 = LinearRegression(labelCol='duration').fit(flights_train_4)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions_3 = regression_3.transform(flights_test_4)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733.822583434675\n",
      "105.16470538246998\n",
      "105.10140374363006\n",
      "105.20472366738178\n"
     ]
    }
   ],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60 / regression_3.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "inter_2 = regression_3.intercept\n",
    "print(inter_2)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_jfk = inter_2 + regression_3.coefficients[1]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "avg_ground_lga = inter_2 + regression_3.coefficients[2]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0024, 0.0068, 0.0062, 0.0099, 0.009, -0.0012, 0.0038, -0.0006])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_3.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "|  9.48|          3.0|\n",
      "| 16.33|          5.0|\n",
      "|  6.17|          2.0|\n",
      "| 10.33|          3.0|\n",
      "|  8.92|          2.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "|  9.48|          3.0|(7,[3],[1.0])|\n",
      "| 16.33|          5.0|(7,[5],[1.0])|\n",
      "|  6.17|          2.0|(7,[2],[1.0])|\n",
      "| 10.33|          3.0|(7,[3],[1.0])|\n",
      "|  8.92|          2.0|(7,[2],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
